---
title: 本地翻译模型部署(个人随笔)
categories: 生活日常
tags:
  - 笔记
  - 随笔
  - 科技
location: 南宁
abbrlink: 9fcec3f
summary: >-
  这篇文章介绍了在个人轻薄本上部署本地翻译模型的完整流程，从硬件准备到系统配置、Ollama安装与优化、浏览器集成以及常见问题解决。内容包括硬件检测方法、显存与运行内存的最低配置建议，以及针对Intel显卡的驱动安装注意事项；详细说明了Ollama的安装路径自定义、模型下载与管理，并推荐了适合翻译和日常问答的模型选择；此外还提供了浏览器插件配置、API设置和性能优化技巧，如环境变量调整以提升GPU效率和节省显存，最后列出了相关资源获取途径，帮助用户高效实现本地翻译功能。
date: 2025-07-12 13:18:47
---

## 一、硬件准备与系统配置

### 1. 硬件要求检测

- [硬件兼容性检测工具](https://tools.thinkinai.xyz/)

- **显存检测方法**：
  1. `Win+R` → 输入 `dxdiag`
  2. 右键桌面 → 显示设置 → 高级显示器设置 → 显示器属性
  3. 任务管理器 → 性能 → GPU

- **最低配置建议**：
  - 共享显存 ≥ 8GB
  - 运行内存 ≥ 16GB
  - Intel ARC/Iris Xe 系列显卡(更符合本人使用轻薄本的情景)

### 2. 驱动更新

- **下载地址**：
  [Intel显卡驱动下载](https://www.intel.cn/content/www/cn/zh/download/785597/intel-arc-iris-xe-graphics-windows.html)
  
- **安装注意事项**：
  ```diff
  - 不要勾选"全新安装"（会清除笔记本OEM设置）
  + 选择"自定义安装"保留现有配置
  ```


### 3. 模型选择建议

| 模型名称                  | 显存要求 | 适用场景          | 性能表现 |
| :------------------------ | :------- | :---------------- | :------- |
| **deepseek-r1:8b**        | 8GB      | 日常问答/文本处理 | ⭐⭐⭐⭐     |
| **deepseek-coder-v2:16b** | 16GB     | 编程/`翻译`       | ⭐⭐⭐⭐⭐    |
| **nomic-embed-text**      | 6GB      | 浏览器增强        | ⭐⭐⭐      |

> 💡 **选择建议**：日常问答推荐 `deepseek-r1:8b`，翻译需求选择 `deepseek-coder-v2:16b`

实际体验下来笔记本能飞8b模型也是勉强



## 二、Ollama 安装与配置

安装[Ollama Intel优化版](https://www.modelscope.cn/models/Intel/ollama),以下内容属于`官网版Ollama`的安装指南内容,下载优化版可以省略以下步骤.

### 1. 自定义安装路径

**操作步骤**：

1. 创建目标目录（如 `D:\apps\Ollama\`）

2. 终端执行安装命令：

   `powershell`

   ```powershell
   # 标准CMD
   OllamaSetup.exe /DIR=D:\apps\Ollama
   
   # PowerShell
   ./OllamaSetup.exe /DIR=D:\apps\Ollama
   ```
### 2. 验证安装

`powershell`

```powershell
cd C:\Users\<用户名>
ollama -v  # 应显示版本号
```

### 3. 模型下载与管理

`powershell`

```powershell
# 下载模型
ollama run deepseek-r1:8b

# 查看已安装模型
ollama list

# 删除模型
ollama rm <模型名>
```


## 三、浏览器集成方案

### 1. Page Assist 插件配置

1. 安装 [Edge Page Assist 扩展](https://microsoftedge.microsoft.com/addons/detail/page-assist-a-web-ui-fo/ogkogooadflifpmmidmhjedogicnhooa),[Chrome Page Assist 拓展](https://chromewebstore.google.com/detail/page-assist-%E6%9C%AC%E5%9C%B0-ai-%E6%A8%A1%E5%9E%8B%E7%9A%84-web/jfgfiigpkhlkbnfnbobbkinehhfdhndo)

2. 打开扩展 → RAG设置 → 文本嵌入模型 → `nomic-embed-text`

3. 下载依赖模型：

   `powershell`

   ```
   ollama pull nomic-embed-text
   ```

### 2. 沉浸式翻译设置

**API 配置**：

```text
API地址：http://localhost:11434/v1/chat/completions
推荐模型：deepseek-coder-v2:16b(主要是省略思考过程)
```
实际体验下来,`qwen3和qwen2.5`也挺适合翻译
**跨域设置**：

| 系统        | 操作                                                         |
| :---------- | :----------------------------------------------------------- |
| **Windows** | 控制面板 → 系统属性 → 环境变量 → 新建： `OLLAMA_HOST=0.0.0.0` `OLLAMA_ORIGINS=*` |
| **macOS**   | 终端执行： `launchctl setenv OLLAMA_ORIGINS "*"`             |
| **Linux**   | 终端执行： `OLLAMA_ORIGINS="*" ollama serve`                 |

## 四、常见问题解决

### 1. 性能优化技巧

#### **性能调优**

你可以尝试如下设置来进行性能调优：[内容引自ModelScope](https://www.modelscope.cn/models/Intel/ollama)

##### 环境变量 `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS`

环境变量 `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS` 用于控制是否使用 immediate command lists 将任务提交到 GPU。你可以尝试将 `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS` 设为 `1` 或 `0` 以找到最佳性能配置。

你可以通过如下步骤，在**启动 Ollama serve 之前**启用 `SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS`（如果 Ollama serve 已经在运行，请确保先将其停止）：

- 对于 **Windows** 用户：

  - 打开命令提示符，并通过 `cd /d PATH\TO\EXTRACTED\FOLDER` 命令进入解压后的文件夹
  - 在命令提示符中设置 `set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1`
  - 通过运行 `start-ollama.bat` 启动 Ollama serve

- 对于 **Linux** 用户：

  - 在终端中输入指令 `cd PATH/TO/EXTRACTED/FOLDER` 进入解压后的文件夹
  - 在终端中设置 `export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1`
  - 通过运行 `./start-ollama.sh` 启动 Ollama serve

#####  **节省 VRAM**

  你可以通过在**启动 Ollama serve 之前**设置环境变量 `OLLAMA_NUM_PARALLEL` 为 `1` 来节约显存，步骤如下（如果 Ollama serve 已经在运行，请确保先将其停止）：

  - 对于 **Windows** 用户：
    - 打开命令提示符，并通过 `cd /d PATH\TO\EXTRACTED\FOLDER` 命令进入解压后的文件夹
    - 在命令提示符中设置 `set OLLAMA_NUM_PARALLEL=1`
    - 通过运行 `start-ollama.bat` 启动 Ollama serve
  - 对于 **Linux** 用户：
    - 在终端中输入指令 `cd PATH/TO/EXTRACTED/FOLDER` 进入解压后的文件夹
    - 在终端中设置 `export OLLAMA_NUM_PARALLEL=1`
    - 通过运行 `./start-ollama.sh` 启动 Ollama serve

  对于 **MoE 模型**（比如 `qwen3:30b`），你可以通过在**启动 Ollama serve 之前**设置环境变量 `OLLAMA_SET_OT` 把 experts 移到 CPU 运行上来节约显存（如果 Ollama serve 已经在运行，请确保先将其停止）：

  - 对于 **Windows** 用户：
    - 打开命令提示符，并通过 `cd /d PATH\TO\EXTRACTED\FOLDER` 命令进入解压后的文件夹
    - 在命令提示符中设置 `set OLLAMA_SET_OT="exps=CPU"` 把所有的 experts 放在 CPU 上；也可以通过设置正则表达式，如 `set OLLAMA_SET_OT="(2[4-9]|[3-9][0-9])\.ffn_.*_exps\.=CPU"` 把 `24` 到 `99` 层的 experts 放到 CPU 上
    - 通过运行 `start-ollama.bat` 启动 Ollama serve
  - 对于 **Linux** 用户：
    - 在终端中输入指令 `cd PATH/TO/EXTRACTED/FOLDER` 进入解压后的文件夹
    - 在终端中设置 `export OLLAMA_SET_OT="exps=CPU"` 把所有的 experts 放在 CPU 上；也可以通过设置正则表达式，如 `export OLLAMA_SET_OT="(2[4-9]|[3-9][0-9])\.ffn_.*_exps\.=CPU"` 把 `24` 到 `99` 层的 experts 放到 CPU 上
    - 通过运行 `./start-ollama.sh` 启动 Ollama serve

### 2. 模型使用建议

{% mermaid %}
A[任务类型] --> B{模型选择}
B -->|日常问答| C[deepseek-r1:8b]
B -->|编程翻译| D[deepseek-coder-v2:16b]
B -->|网页增强| E[nomic-embed-text]
{% endmermaid %}


### 3. 资源获取

- [Intel优化版Ollama](https://www.modelscope.cn/models/Intel/ollama)
- [硬件兼容性检测工具](https://tools.thinkinai.xyz/)
- [官方文档](https://ollama.com/)